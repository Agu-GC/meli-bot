# ğŸ¤– `meli-chatbot`
# Chatbot de Seguridad con RAG y Ollama

**`meli-bot`** es una Prueba de Concepto (POC) de un chatbot conversacional diseÃ±ado para responder preguntas sobre documentaciÃ³n de seguridad del equipo de seguridad. Utiliza RecuperaciÃ³n Aumentada por GeneraciÃ³n (RAG), almacenamiento semÃ¡ntico vectorial con **ChromaDB**, modelos LLM locales administrados por **Ollama**, y sigue principios de **Clean Architecture** para facilitar su mantenibilidad y escalabilidad.

<br/>
---


## ğŸ“Œ PropÃ³sito

El chatbot tiene como objetivo asistir en consultas frecuentes del equipo de seguridad sobre temas como autenticaciÃ³n, autorizaciÃ³n y normativas internas. La documentaciÃ³n se almacena en archivos locales y es embebida al iniciar la aplicaciÃ³n, permitiendo bÃºsquedas semÃ¡nticas rÃ¡pidas y respuestas contextualizadas.

<br/>
---


## ğŸ§© Componentes principales

| Componente                | Rol en el sistema                                                                                                                       |
| ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |
| **FastAPI REST API**      | Expone un Ãºnico endpoint `/chat`, que recibe mensajes del usuario y entrega respuestas. Al iniciar, tambiÃ©n carga documentos embebidos. |
| **Redis**                 | Almacena las Ãºltimas 10 interacciones por `user_id` para mantener el contexto de la conversaciÃ³n.                                       |
| **ChromaDB**              | Base de datos vectorial donde se almacenan los embeddings de los documentos cargados dinÃ¡micamente desde la carpeta `./documents`.      |
| **LLM con Ollama**        | Permite utilizar cualquier modelo compatible con Ollama.                          |
| **Sentence Transformers** | Se usa en tiempo real para codificar la consulta del usuario y los documentos.                                                          |
| **Docker Compose**        | Orquesta los servicios de API, Redis y Chroma. La configuraciÃ³n se maneja vÃ­a variables de entorno.                                     |

<br/>
---


## âš™ï¸ Flujo de ejecuciÃ³n

### ğŸ”„ Al iniciar la API:

1. Escanea la carpeta `./documents`.
2. Valida quÃ© archivos aÃºn no han sido embebidos en Chroma.
3. Divide cada nuevo documento en fragmentos.
4. Genera los embeddings con `sentence-transformers`.
5. Guarda los vectores en Chroma.

### ğŸ’¬ Al recibir un mensaje:

1. Recupera las Ãºltimas 10 interacciones del usuario desde Redis.
2. Codifica la consulta del usuario.
3. Busca los documentos mÃ¡s similares en Chroma.
4. Construye un prompt con: historial + resultados de bÃºsqueda + consulta del usuario.
5. Llama al modelo a travÃ©s del `LLMService` que se comunica con `ollama`.
6. Guarda la interacciÃ³n en Redis y devuelve la respuesta.

<br/>
---


## ğŸ§± Estructura del proyecto (Clean Architecture)

```
meli-bot/
â”‚
â””â”€â”€â”€api
â”‚   â”œâ”€â”€ domain/                     # Entidades principales del sistema
â”‚   â”‚   â””â”€â”€ entities.py
â”‚   â”‚
â”‚   â”œâ”€â”€ application/
â”‚   â”‚   â”œâ”€â”€ services/               # LÃ³gica de negocio principal
â”‚   â”‚   â””â”€â”€ interfaces/             # Interfaces de entrada/salida
â”‚   â”‚
â”‚   â”œâ”€â”€ infrastructure/
â”‚   â”‚   â”œâ”€â”€ repositories/           # Acceso a Redis y Chroma
â”‚   â”‚   â”œâ”€â”€ services/               # Embeddings, LLM (Ollama)
â”‚   â”‚   â””â”€â”€ web/                    # Endpoints de FastAPI
â”‚   â””â”€â”€ main.py                     # Punto de entrada
â”‚
â”œâ”€â”€ documents/                  # Documentos que se embeben en Chroma
â”œâ”€â”€ Dockerfile                  # CreaciÃ³n de imagen de la API REST
â”œâ”€â”€ docker-compose.yml          # OrquestaciÃ³n de servicios
â”œâ”€â”€ script_benchmark.sh         # Script para comparacion de modelos
â”œâ”€â”€ script_cleanup.sh           # Script para limpiar el host
â”œâ”€â”€ script_stratup.sh           # Script para iniciar el Chatbot
â””â”€â”€ .env                        # ConfiguraciÃ³n del entorno
```


---


## ğŸ“¦ CÃ³mo ejecutar

### 1. Requisitos

* Docker + Docker Compose
* Brew instalado localmente

### 2. Configurar variables en `.env` (ejemplo)

```env
OLLAMA_MODEL="gemma3:1b-it-q8_0"
OLLAMA_MODEL_PROMPT_FORMAT="<start_of_turn>user\\n{prompt}<end_of_turn><start_of_turn>model"
```

### 3. Levantar servicios

```bash
sh script_startup.sh
```

1. Validara que se tenga `ollama` instalado
2. Iniciara `ollama serve`
2. DescargarÃ¡ el modelo necesario
3. CorrerÃ¡ `docker compose up --build` construyendo la imagen de la API y descargando las demÃ¡s. Levantando *REDIS*, *CHROMA* y el *API REST*

### 4. Probar el endpoint

```bash
curl --location 'http://0.0.0.0:8001/api/v1/chat' \
--header 'Content-Type: application/json' \
--data '{
    "user_id": "jose",
    "message": "que es oauth"
  }'
```

#### Algunas Pruebas


```bash
curl --location 'http://0.0.0.0:8001/api/v1/chat' \
--header 'Content-Type: application/json' \
--data '{
    "user_id": "jose",
    "message": "What is OAuth2?"
  }'
```

```json
{
       "response": "OAuth2 is an authorization framework that allows third-party applications to access resources on an user's behalf without requiring the user to share their credentials. It provides secure and standardized methods for obtaining access tokens, which can be used to make API requests on the user's behalf."
}
```


```bash
curl --location 'http://0.0.0.0:8001/api/v1/chat' \
--header 'Content-Type: application/json' \
--data '{
    "user_id": "jose",
    "message": "Who create it?"
  }'
```

```json
{
    "response": "OAuth2 was developed by a group of industry experts and was approved as a standard by IETF (Internet Engineering Task Force) in August 2012. It is an update to the original OAuth protocol, providing improved security features and more flexible authorization capabilities."
}
```


```bash
curl --location 'http://0.0.0.0:8001/api/v1/chat' \
--header 'Content-Type: application/json' \
--data '{
    "user_id": "miguel",
    "message": "Â¿puedes responder en espanol?"
  }'
```

```json
{
    "response": "SÃ­, soy un asistente capaz de responder preguntas basadas en el contexto proporcionado. El proceso de reconciliaciÃ³n implica que eventos se desencadenan y se procesan segÃºn polÃ­ticas definidas. La polÃ­tica de administraciÃ³n de usuarios es importante para determinar quiÃ©n debe tener acceso a quÃ© sistemas y quÃ© tipos de permisos y privilegios tienen. El mecanismo Challenge-Response es una forma de comprobar la identidad, donde el solicitudante de acceso se desafÃ­a por parte del proveedor de acceso y debe responder segÃºn lo esperado. Por ejemplo, preguntas secretas o certificados digitales. El catalogo de permisos es una base de datos de permisos y sus metadatos.\n\nNo entiendo la pregunta sobre el espaÃ±ol especÃ­ficamente en este contexto. Si estÃ¡ relacionada con idiomas, por favor, clarifique la pregunta."
}
```

### 5. Limpiar ambiente

```bash
sh script_cleanup.sh
```

<br/>
---


## ğŸ“š Benchmark

Se realizaron pruebas con los principales modelos ligeros recomendados para host con M1 (como lo son nuestras PCs)  
A traves del script `script_benchmark.sh` se analizaron:  
- "llama3:8b-instruct-q4_K_M"
- "llama3:8b-instruct-q5_K_M"
- "mistral:7b-instruct-v0.2-q4_K_M"
- "mistral:7b-instruct-v0.2-q5_K_M"
- "deepseek-coder:6.7b-instruct-q4_K_M"
- "deepseek-llm:7b-chat-q4_K_M"
- "phi3:3.8b-mini-128k-instruct-q5_K_M"
- "phi3:14b-medium-128k-instruct-q4_K_M"
- "gemma3:4b-it-q4_K_M"
- "gemma3:1b-it-q4_K_M"
- "gemma3:1b-it-q8_0"
Los resultados fueron almacenados en el file `benchmark_results.txt`.  
En caso de querer analizar otros modelos solo se debe incluir la version del modelo en la variable MODELS y el template del prompt esperado en PROMPTS_FORMAT del script

<br/>
---


## ğŸ“˜ Decisiones tÃ©cnicas

* **RAG sobre almacenamiento local:** permite respuesta contextual sin necesidad de modelos muy grandes ni APIs externas. Evita el reentrenamiento del modelo. Y es mas facil de expandir.
* **LLM local:** se eligiÃ³ Ollama para poder probar mÃºltiples modelos como LLaMA3, Mistral, Phi-3, DeepSeek sin depender de terceros. Se intento levantar Ollama como contenedor pero no mostrÃ³ buena performance, agergaba mucha latencia a las consultas.
* **Ollama:** modelos intercambiables. Flexibilidad para experimentar
* **Embeddings en tiempo real:** otorga mÃ¡xima flexibilidad y evita preprocesamiento manual.
* **Clean Architecture:** asegura una estructura sÃ³lida y desacoplada, ideal para iteraciones futuras o escalar a producciÃ³n.
* **Vector DB - ChromaDB:** FÃ¡cil integraciÃ³n, ligero y Open Source.
* **Almacenamiento Historial - Redis:** Baja latencia para acceso conversacional

<br/>
---


## ğŸ“ˆ Mejoras pendientes

* ğŸ” AutenticaciÃ³n de los requests
* ğŸ—ƒï¸ Testing


---