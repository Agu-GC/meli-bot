#!/bin/bash
set -e

MODELS=(
  "llama3:8b-instruct-q4_K_M"
  "llama3:8b-instruct-q5_K_M"
  "mistral:7b-instruct-v0.2-q4_K_M"
  "mistral:7b-instruct-v0.2-q5_K_M"
  "deepseek-coder:6.7b-instruct-q4_K_M"
  "deepseek-llm:7b-chat-q4_K_M"
  "phi3:3.8b-mini-128k-instruct-q5_K_M"
  "phi3:14b-medium-128k-instruct-q4_K_M"
  "gemma3:4b-it-q4_K_M"
  "gemma3:1b-it-q4_K_M"
  "gemma3:1b-it-q8_0"
)
PROMPTS_FORMAT=(
  "<|start_header_id|>system<|end_header_id|><|start_header_id|>user<|end_header_id|>\\n{prompt}\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>"
  "<|start_header_id|>system<|end_header_id|><|start_header_id|>user<|end_header_id|>\\n{prompt}\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>"
  "[INST]\\n{prompt}\\n[/INST]"
  "[INST]\\n{prompt}\\n[/INST]"
  "User:{prompt}Assistant:"
  "User:{prompt}Assistant:"
  "<|system|><|end|><|user|>\\n{prompt}<|end|><|assistant|>"
  "<|system|><|end|><|user|>\\n{prompt}<|end|><|assistant|>"
  "<start_of_turn>user\\n{prompt}<end_of_turn><start_of_turn>model"
  "<start_of_turn>user\\n{prompt}<end_of_turn><start_of_turn>model"
  "<start_of_turn>user\\n{prompt}<end_of_turn><start_of_turn>model"
)




# =============================================
# 1. VERIFICACIÃ“N E INSTALACIÃ“N DE OLLAMA
# =============================================
echo "ðŸ” Verificando instalaciÃ³n de Ollama..."

if [[ ! -f "/usr/local/bin/ollama" && ! -f "/Applications/Ollama.app/Contents/Resources/ollama" ]]; then
    echo "âš™ï¸ Ollama no estÃ¡ instalado. Instalando..."
    brew install ollama || {
        echo "âŒ Error: No se pudo instalar Ollama con Homebrew. Por favor, instÃ¡lalo manualmente."
        exit 1
    }
    echo "âœ… Ollama instalado correctamente"
fi

# =============================================
# 2. INICIO DEL SERVICIO
# =============================================
echo "ðŸš€ Iniciando servicio Ollama..."
export OLLAMA_USE_METAL=1
export OLLAMA_FLASH_ATTENTION=1

if ! curl -s http://localhost:11434/api/tags &> /dev/null; then
    echo "ðŸ”Œ Iniciando Ollama en segundo plano..."
    nohup ollama serve > /tmp/ollama.log 2>&1 &
    
    timeout=30
    while [ $timeout -gt 0 ]; do
        if curl -s http://localhost:11434/api/tags &> /dev/null; then
            break
        fi
        sleep 1
        ((timeout--))
    done
    
    if [ $timeout -eq 0 ]; then
        echo "âŒ Error: Timeout al iniciar Ollama. Ver logs en /tmp/ollama.log"
        echo "â„¹ï¸ Intenta iniciar Ollama manualmente: ollama serve"
        exit 1
    fi
else
    echo "âœ… Ollama ya estÃ¡ en ejecuciÃ³n"
fi

# =============================================
# 3. DESCARGAR MODELOS 
# =============================================
echo ""
echo "ðŸ”½ Descargando modelos..."
for MODEL in "${MODELS[@]}"; do
  echo "âž¡ï¸  ollama pull $MODEL"
  ollama pull "$MODEL"
done

# =============================================
# 4. INICIAR BENCHMARK
# =============================================
echo ""
echo "ðŸ§ª Benchmark iniciado..."
BENCH_FILE="benchmark_results.txt"
echo "Fecha: $(date)" > "$BENCH_FILE"
echo "Prompt: $PROMPT" >> "$BENCH_FILE"
echo "" >> "$BENCH_FILE"

# Recorrer usando Ã­ndice numÃ©rico
for (( i=0; i<${#MODELS[@]}; i++ )); do
  MODEL=${MODELS[$i]}
  PROMPT=${PROMPTS_FORMAT[$i]}
  
  export OLLAMA_MODEL="$MODEL"
  export OLLAMA_PROMPT="$PROMPT_FORMAT"
  docker-compose up --build -d

  echo "â³ Esperando que la API estÃ© lista..."
  
  timeout=30
  while ! curl -s http://localhost:8001/health > /dev/null; do
    sleep 1
    ((timeout--))
    if [ $timeout -eq 0 ]; then
      echo "âŒ Timeout esperando el servicio en http://localhost:8001"
      exit 1
    fi
  done
  
  echo "âœ… API disponible. Ejecutando prueba..."


  START=$(date +%s)
  RESPONSE=$(curl --location 'http://0.0.0.0:8001/api/v1/chat' --header 'Content-Type: application/json' --data '{"user_id": "miguel","message": "Â¿que es Oauth2?"}')
  END=$(date +%s)
  DURATION=$(( (END - START) * 1000 ))

  echo "=== $MODEL ===" >> "$BENCH_FILE"
  echo "â±ï¸ Tiempo de respuesta: ${DURATION} ms" >> "$BENCH_FILE"
  echo "ðŸ“¥ Respuesta (primeras lÃ­neas):" >> "$BENCH_FILE"
  echo "$RESPONSE" | head -n 10 >> "$BENCH_FILE"
  echo "" >> "$BENCH_FILE"

  echo "âœ… Modelo $MODEL completado en ${DURATION} ms"
  echo "----------------------------------------"
  
  docker-compose down
done

echo "âœ… Benchmark completado. Resultados guardados en $BENCH_FILE"
